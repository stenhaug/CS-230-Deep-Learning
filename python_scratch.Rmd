---
title: "CoRe_Python_mainframe"
---

```{r}
library(reticulate)
use_condaenv("CoRe_python", required = TRUE)
```

```{python}
import numpy as np

A = np.random.randn(4,3)
B = np.sum(A, axis = 1, keepdims = True)

```

```{python}
def sigmoid(z):
   s = 1 / (1 + np.exp(-z))
   return s
```


```{python}
def propagate(w, b, X, Y):
   """
   Implement the cost function and its gradient for the propagation explained above
   
   Arguments:
   w -- weights, a numpy array of size (num_px * num_px * 3, 1)
   b -- bias, a scalar
   X -- data of size (num_px * num_px * 3, number of examples)
   Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)
   
   Return:
   cost -- negative log-likelihood cost for logistic regression
   dw -- gradient of the loss with respect to w, thus same shape as w
   db -- gradient of the loss with respect to b, thus same shape as b
   
   Tips:
   - Write your code step by step for the propagation. np.log(), np.dot()
   """
   
   m = X.shape[1]
   
   # FORWARD PROPAGATION (FROM X TO COST)
   ### START CODE HERE ### (≈ 2 lines of code)
   A = sigmoid(np.dot(w.T, X) + b)             # compute activation
   cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / X.shape[1]  # compute cost
   ### END CODE HERE ###
   
   # BACKWARD PROPAGATION (TO FIND GRAD)
   ### START CODE HERE ### (≈ 2 lines of code)
   dw = np.dot(X, (A - Y).T) / X.shape[1]
   db = np.sum(A - Y) / X.shape[1]
   ### END CODE HERE ###
   
   assert(dw.shape == w.shape)
   assert(db.dtype == float)
   cost = np.squeeze(cost)
   assert(cost.shape == ())
   
   grads = {"dw": dw,
          "db": db}
   
   return grads, cost
```

```{python}
w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])

learning_rate = 0.009

grads, cost = propagate(w, b, X, Y)

dw = grads["dw"]
db = grads["db"]

dw

db

w = w - learning_rate * dw

w

A = sigmoid(np.dot(w.T, X) + b)

A[2, 1]

```

```{python}
m = X.shape[1]
Y_prediction = np.zeros((1,m))
w = w.reshape(X.shape[0], 1)

A = sigmoid(np.dot(w.T, X) + b)

A.shape[1]

A[]

for i in range(A.shape[1]):
   if A[1, i] > 0.5:
      Y_prediction[1, i] = 1

```


